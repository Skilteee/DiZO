TASK: SST-2
K: 16
Seed: 42
BS: 64
LR: 1e-6
EPS: 1e-3
Step: 100000; Eval step: 10000
Grid search tag: seed42-bs64-lr1e-6-eps1e-3-wd0-step100000-evalstep10000
Tag: k16-roberta-large-mezo-ft
2025-03-31 18:36:39.303334: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-03-31 18:36:39.305534: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-31 18:36:39.310359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1743460599.318205 1238579 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1743460599.320711 1238579 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-31 18:36:39.328928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
03/31/2025 18:36:40 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
03/31/2025 18:36:40 - INFO - __main__ -   Training/evaluation parameters DynamicTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
adjust_for_init=False,
array_id=-1,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
binary_classification=False,
change_grad_estimate=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=True,
do_train=True,
efficient_zero_order=True,
efficient_zero_order_fp16=False,
enhanced=None,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=10000,
evaluate_during_training=True,
evaluation_strategy=no,
exclude_embeddings=False,
exclude_first_layers=-1,
exclude_head=False,
f0_scaling=1.0,
fix_layers=0,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
from_linearhead=False,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
head_tuning=False,
hf_inference_model=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
kernel_formula=sgd,
kernel_gamma=1.0,
kernel_regularization=0.0,
kernel_solver=logistic,
label_names=None,
label_smoothing_factor=0.0,
layer_wise_optim=False,
learning_rate=1e-06,
length_column_name=length,
load_best_model_at_end=False,
load_kernels=None,
local_rank=-1,
log_file=log,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=result/SST-2-roberta-large-prompt-standard-k16-roberta-large-mezo-ftseed42-bs64-lr1e-6-eps1e-3-wd0-step100000-evalstep10000/16-42/runs/Mar31_18-36-40_BGCSCI-734472,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lp_early_stopping=False,
lr_scheduler_type=constant,
max_grad_norm=1.0,
max_steps=100000,
max_zo_forward_steps=0,
mc_tol=0.1,
metric_for_best_model=None,
model_id=-1,
mp_parameters=,
no_cuda=False,
no_predict=False,
no_reparam=False,
no_train=False,
norm_running_update=False,
num_hvp_vecs=128,
num_prefix=10,
num_train_epochs=3.0,
only_biases=False,
optim=adamw_hf,
optim_args=None,
optimize_acc=False,
optimizer=sgd,
optimizer_variant=,
output_dir=result/SST-2-roberta-large-prompt-standard-k16-roberta-large-mezo-ftseed42-bs64-lr1e-6-eps1e-3-wd0-step100000-evalstep10000/16-42,
overwrite_kernels=False,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=64,
prediction_loss_only=False,
prefix_init_by_real_act=False,
prefix_tuning=False,
prob_as_feature=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
random_model_init=False,
ray_scope=last,
recompute_norms=False,
remove_unused_columns=True,
report_to=['tensorboard', 'wandb'],
resume_from_checkpoint=None,
run_name=result/SST-2-roberta-large-prompt-standard-k16-roberta-large-mezo-ftseed42-bs64-lr1e-6-eps1e-3-wd0-step100000-evalstep10000/16-42,
save_at_last=False,
save_logit=False,
save_logit_dir=None,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
scale_lr_with_samples=False,
scale_norm_by_num_params=False,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sweep=False,
sync_embedding_layers=False,
tf32=None,
tie_emb=False,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
trainer=standard,
untie_emb=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
use_zo_grad_est=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
zero_order_clip_grad=False,
zero_order_eps=0.001,
zero_order_optim=True,
zero_order_sample=1,
zero_order_sample_scheduler=None,
zero_order_use_trainer_optim=False,
zo_by_layer=False,
zo_variant=None,
)
03/31/2025 18:36:40 - INFO - __main__ -   Task name: sst-2, number of labels: 2, output mode: classification
/home/qitao/lib/python3.10/site-packages/transformers/modeling_utils.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location="cpu")
03/31/2025 18:36:40 - WARNING - src.models -   By default for RoBERTa models the input embeddings and the output embeddings are NOT tied!!!!
RobertaConfig {
  "_name_or_path": "/home/qitao/PeZO-main-raw/Divergence-driven-Zeroth-order/medium_models/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "finetuning_task": "sst-2",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.28.1",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

Some weights of RobertaModelForPromptFinetuning were not initialized from the model checkpoint at /home/qitao/PeZO-main-raw/Divergence-driven-Zeroth-order/medium_models/roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias', 'roberta.embeddings.position_ids']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
03/31/2025 18:36:42 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)
03/31/2025 18:36:42 - INFO - src.dataset -   Label 1 to word Ġgreat (372)
03/31/2025 18:36:42 - INFO - src.dataset -   Total num_sample for mode train: 1
03/31/2025 18:36:42 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/16-42
/home/qitao/PeZO-main-raw/Divergence-driven-Zeroth-order/medium_models/src/dataset.py:353: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  self.support_examples, self.query_examples = torch.load(cached_features_file)
03/31/2025 18:36:42 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/16-42/cached_train_RobertaTokenizerFast-roberta_128_sst-2 [took 0.000 s]
03/31/2025 18:36:42 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)
03/31/2025 18:36:42 - INFO - src.dataset -   Label 1 to word Ġgreat (372)
03/31/2025 18:36:42 - INFO - src.dataset -   Total num_sample for mode dev: 1
03/31/2025 18:36:42 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/16-42
03/31/2025 18:36:42 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/16-42/cached_dev_RobertaTokenizerFast-roberta_128_sst-2 [took 0.000 s]
03/31/2025 18:36:42 - INFO - src.dataset -   *** Example ***
03/31/2025 18:36:42 - INFO - src.dataset -   guid: dev-1
03/31/2025 18:36:42 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 571, 4097, 82, 22107, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=0, mask_pos=[8], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)
03/31/2025 18:36:42 - INFO - src.dataset -   text: <s>gave people seizures  It was<mask>.</s>
03/31/2025 18:36:42 - INFO - src.dataset -   Label 0 to word Ġterrible (6587)
03/31/2025 18:36:42 - INFO - src.dataset -   Label 1 to word Ġgreat (372)
03/31/2025 18:36:42 - INFO - src.dataset -   Total num_sample for mode test: 1
03/31/2025 18:36:42 - INFO - src.dataset -   Creating/loading examples from dataset file at data/k-shot-1k-test/SST-2/16-42
03/31/2025 18:36:42 - INFO - src.dataset -   Loading features from cached file data/k-shot-1k-test/SST-2/16-42/cached_test_RobertaTokenizerFast-roberta_128_sst-2 [took 0.001 s]
03/31/2025 18:36:42 - INFO - src.dataset -   *** Example ***
03/31/2025 18:36:42 - INFO - src.dataset -   guid: test-1
03/31/2025 18:36:42 - INFO - src.dataset -   features: OurInputFeatures(input_ids=[0, 405, 128, 29, 10, 18452, 8, 747, 7920, 3251, 479, 1437, 85, 21, 50264, 4, 2], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=None, label=1, mask_pos=[14], label_word_list=None, sfc_input_ids=None, sfc_attention_mask=None, sfc_mask_pos=None)
03/31/2025 18:36:42 - INFO - src.dataset -   text: <s>it's a charming and often affecting journey.  It was<mask>.</s>
03/31/2025 18:36:42 - INFO - src.trainer -   ***** Running training *****
03/31/2025 18:36:42 - INFO - src.trainer -     Num examples = 32
03/31/2025 18:36:42 - INFO - src.trainer -     Num Epochs = 100000
03/31/2025 18:36:42 - INFO - src.trainer -     Instantaneous batch size per device = 64
03/31/2025 18:36:42 - INFO - src.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 64
03/31/2025 18:36:42 - INFO - src.trainer -     Gradient Accumulation steps = 1
03/31/2025 18:36:42 - INFO - src.trainer -     Total optimization steps = 100000
03/31/2025 18:36:42 - INFO - src.trainer -     Starting fine-tuning.
task name: SST-2
You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: qitaotan02 (qitaotan02-ancestry) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /home/qitao/PeZO-main-raw/Divergence-driven-Zeroth-order/medium_models/wandb/run-20250331_183643-syqhjog7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run spring-disco-184
wandb: ⭐️ View project at https://wandb.ai/qitaotan02-ancestry/huggingface
wandb: 🚀 View run at https://wandb.ai/qitaotan02-ancestry/huggingface/runs/syqhjog7
03/31/2025 18:36:43 - INFO - src.trainer -   {'loss': 0.46644309163093567, 'global_step': 0, 'zo_forward_step': 2, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 0}
03/31/2025 18:36:48 - INFO - src.trainer -   {'loss': 0.3975989520549774, 'global_step': 50, 'zo_forward_step': 102, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 5}
03/31/2025 18:36:53 - INFO - src.trainer -   {'loss': 0.41928160190582275, 'global_step': 100, 'zo_forward_step': 202, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 10}
03/31/2025 18:36:58 - INFO - src.trainer -   {'loss': 0.38310569524765015, 'global_step': 150, 'zo_forward_step': 302, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 15}
03/31/2025 18:37:02 - INFO - src.trainer -   {'loss': 0.3721981942653656, 'global_step': 200, 'zo_forward_step': 402, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 19}
03/31/2025 18:37:07 - INFO - src.trainer -   {'loss': 0.28935566544532776, 'global_step': 250, 'zo_forward_step': 502, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 24}
03/31/2025 18:37:12 - INFO - src.trainer -   {'loss': 0.3021770119667053, 'global_step': 300, 'zo_forward_step': 602, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 29}
03/31/2025 18:37:17 - INFO - src.trainer -   {'loss': 0.3228414058685303, 'global_step': 350, 'zo_forward_step': 702, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 34}
/home/qitao/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:411: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 210/218 [00:01<00:00, 148.92it/s]/home/qitao/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:61: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
/home/qitao/lib/python3.10/site-packages/transformers/data/metrics/__init__.py:31: FutureWarning: This metric will be removed from the library soon, metrics should be handled with the 🤗 Evaluate library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py
  warnings.warn(DEPRECATION_WARNING, FutureWarning)
03/31/2025 18:37:23 - INFO - src.trainer -   {'eval_loss': 0.32075387239456177, 'eval_acc': 0.8681192660550459}
03/31/2025 18:37:23 - INFO - src.trainer -   Best dev result: 0.8681192660550459
03/31/2025 18:37:23 - INFO - src.trainer -   {'loss': 0.271246999502182, 'global_step': 400, 'zo_forward_step': 802, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 40}
03/31/2025 18:37:28 - INFO - src.trainer -   {'loss': 0.30527958273887634, 'global_step': 450, 'zo_forward_step': 902, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 45}
03/31/2025 18:37:33 - INFO - src.trainer -   {'loss': 0.26923760771751404, 'global_step': 500, 'zo_forward_step': 1002, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 50}
03/31/2025 18:37:37 - INFO - src.trainer -   {'loss': 0.2550228238105774, 'global_step': 550, 'zo_forward_step': 1102, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 54}
03/31/2025 18:37:42 - INFO - src.trainer -   {'loss': 0.22305278480052948, 'global_step': 600, 'zo_forward_step': 1202, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 59}
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 218/218 [00:21<00:00, 148.92it/s]03/31/2025 18:37:47 - INFO - src.trainer -   {'loss': 0.20939205586910248, 'global_step': 650, 'zo_forward_step': 1302, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 64}
03/31/2025 18:37:52 - INFO - src.trainer -   {'loss': 0.23154962062835693, 'global_step': 700, 'zo_forward_step': 1402, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 69}
03/31/2025 18:37:56 - INFO - src.trainer -   {'loss': 0.22345881164073944, 'global_step': 750, 'zo_forward_step': 1502, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 73}
427it [00:41, 72.94it/s]03/31/2025 18:38:03 - INFO - src.trainer -   {'eval_loss': 0.2897413671016693, 'eval_acc': 0.8853211009174312}                                                                                             
03/31/2025 18:38:03 - INFO - src.trainer -   Best dev result: 0.8853211009174312
03/31/2025 18:38:03 - INFO - src.trainer -   {'loss': 0.22385363280773163, 'global_step': 800, 'zo_forward_step': 1602, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 80}
03/31/2025 18:38:08 - INFO - src.trainer -   {'loss': 0.17843230068683624, 'global_step': 850, 'zo_forward_step': 1702, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 85}
436it [00:51, 72.94it/s]03/31/2025 18:38:13 - INFO - src.trainer -   {'loss': 0.17236298322677612, 'global_step': 900, 'zo_forward_step': 1802, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 90}
03/31/2025 18:38:17 - INFO - src.trainer -   {'loss': 0.18321917951107025, 'global_step': 950, 'zo_forward_step': 1902, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 94}
03/31/2025 18:38:22 - INFO - src.trainer -   {'loss': 0.1645640879869461, 'global_step': 1000, 'zo_forward_step': 2002, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 99}
03/31/2025 18:38:27 - INFO - src.trainer -   {'loss': 0.16753211617469788, 'global_step': 1050, 'zo_forward_step': 2102, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 104}
03/31/2025 18:38:32 - INFO - src.trainer -   {'loss': 0.1268458515405655, 'global_step': 1100, 'zo_forward_step': 2202, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 109}
03/31/2025 18:38:36 - INFO - src.trainer -   {'loss': 0.1628231555223465, 'global_step': 1150, 'zo_forward_step': 2302, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 113}
645it [01:21, 72.86it/s]03/31/2025 18:38:42 - INFO - src.trainer -   {'eval_loss': 0.27636823058128357, 'eval_acc': 0.8910550458715596}
03/31/2025 18:38:42 - INFO - src.trainer -   Best dev result: 0.8910550458715596
03/31/2025 18:38:43 - INFO - src.trainer -   {'loss': 0.14361675083637238, 'global_step': 1200, 'zo_forward_step': 2402, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 120}
03/31/2025 18:38:47 - INFO - src.trainer -   {'loss': 0.12758688628673553, 'global_step': 1250, 'zo_forward_step': 2502, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 124}
03/31/2025 18:38:52 - INFO - src.trainer -   {'loss': 0.11865349858999252, 'global_step': 1300, 'zo_forward_step': 2602, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 129}
654it [01:31, 72.86it/s]03/31/2025 18:38:57 - INFO - src.trainer -   {'loss': 0.11911160498857498, 'global_step': 1350, 'zo_forward_step': 2702, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 134}
03/31/2025 18:39:02 - INFO - src.trainer -   {'loss': 0.11230948567390442, 'global_step': 1400, 'zo_forward_step': 2802, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 139}
03/31/2025 18:39:06 - INFO - src.trainer -   {'loss': 0.12073445320129395, 'global_step': 1450, 'zo_forward_step': 2902, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 143}
03/31/2025 18:39:11 - INFO - src.trainer -   {'loss': 0.10427816212177277, 'global_step': 1500, 'zo_forward_step': 3002, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 148}
03/31/2025 18:39:16 - INFO - src.trainer -   {'loss': 0.1230602115392685, 'global_step': 1550, 'zo_forward_step': 3102, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 153}
863it [02:00, 72.98it/s]03/31/2025 18:39:22 - INFO - src.trainer -   {'eval_loss': 0.2633363902568817, 'eval_acc': 0.8990825688073395}
03/31/2025 18:39:22 - INFO - src.trainer -   Best dev result: 0.8990825688073395
03/31/2025 18:39:22 - INFO - src.trainer -   {'loss': 0.09372968971729279, 'global_step': 1600, 'zo_forward_step': 3202, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 159}
03/31/2025 18:39:27 - INFO - src.trainer -   {'loss': 0.0995108112692833, 'global_step': 1650, 'zo_forward_step': 3302, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 164}
03/31/2025 18:39:32 - INFO - src.trainer -   {'loss': 0.07703574746847153, 'global_step': 1700, 'zo_forward_step': 3402, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 169}
872it [02:11, 72.98it/s]03/31/2025 18:39:37 - INFO - src.trainer -   {'loss': 0.09526026248931885, 'global_step': 1750, 'zo_forward_step': 3502, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 174}
03/31/2025 18:39:41 - INFO - src.trainer -   {'loss': 0.07427110522985458, 'global_step': 1800, 'zo_forward_step': 3602, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 178}
03/31/2025 18:39:46 - INFO - src.trainer -   {'loss': 0.062287043780088425, 'global_step': 1850, 'zo_forward_step': 3702, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 183}
03/31/2025 18:39:51 - INFO - src.trainer -   {'loss': 0.0649740919470787, 'global_step': 1900, 'zo_forward_step': 3802, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 188}
03/31/2025 18:39:56 - INFO - src.trainer -   {'loss': 0.048690538853406906, 'global_step': 1950, 'zo_forward_step': 3902, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 193}
1081it [02:40, 73.05it/s]03/31/2025 18:40:02 - INFO - src.trainer -   {'eval_loss': 0.2582210600376129, 'eval_acc': 0.8990825688073395}
03/31/2025 18:40:02 - INFO - src.trainer -   {'loss': 0.04850498214364052, 'global_step': 2000, 'zo_forward_step': 4002, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 199}
03/31/2025 18:40:07 - INFO - src.trainer -   {'loss': 0.04543425887823105, 'global_step': 2050, 'zo_forward_step': 4102, 'max_steps': 100000, 'max_zo_forward_steps': 0, 'time': 204}
